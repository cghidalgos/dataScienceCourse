{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Red de Monitoreo de Calidad del Aire de Bogota.\n",
    "http://rmcab.ambientebogota.gov.co/Report/stationreport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decripción del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Instalar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip # se actualiza pip, en caso de que no lo tenga actualizado\n",
    "#!pip install psycopg2-binary # Psycopg es un adaptador de base de datos PostgreSQL\n",
    "#!pip install pandas \n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import walk\n",
    "import os, re   #expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Procesar conjunto de datos (Preprocesssing-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unir todos los conjuntos de datos de extensión xlsx en un arreglo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unir todos los conjuntos de datos de extensión xlsx en un arreglo \n",
    "arregloDeDataSets = []\n",
    "for (dirpath, dirnames, filenames) in walk('data/raw/'):\n",
    "    arregloDeDataSets.extend(filenames)\n",
    "    break\n",
    "arregloDeDataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  Concatenar los datsets en uno solo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('data/raw/') \n",
    "files = os.listdir(path)  \n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    if file.endswith('.xlsx'):\n",
    "        dft = pd.read_excel('data/raw/'+file)\n",
    "        dft['Station'] = re.search(r'_(.*?)_', str(file)).group(1)\n",
    "        df = df.append(dft[1:], ignore_index=True) \n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Comprobar los valores nulos por columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contar cuantos valores nulos hay por cada columna\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Organizar y guardar en un nuevo df las columnas significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organizar y guardar en un nuevo df las columnas significativas\n",
    "df = df[['PM10','PM2.5','NO','NO2','NOX','CO','OZONO','Station', 'DateTime']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Convertir valores a numericos y sino poner NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['PM10', 'PM2.5', 'NO', 'NO2', 'NOX', 'CO', 'OZONO']\n",
    "for i in col:\n",
    "    df[i] = pd.to_numeric(df[i],errors='coerce') #  el análisis no válido se establecerá como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Ver el tamaño del dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver el tamaño del conjunto de datos \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Crear una nueva columna con filtro (False o True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True is greater than 12, bad (Pure, Not Pure)\n",
    "df['Status'] = df['PM2.5']>12\n",
    "df.Status.value_counts()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Guardar todo el proceso en un nuevo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need another column to specify stations\n",
    "#df.to_csv(\"data/clean_data_final.csv\", index = False)\n",
    "df.to_csv(\"data/dataset_with_missing.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Integrar nuevo dataset (Stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Cargar dataset de stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar el df dataset_with_missing.csv\n",
    "df = pd.read_csv('data/dataset_with_missing.csv')\n",
    "#cargar el df stations_loc.csv\n",
    "stations = pd.read_csv('data/stations_loc.csv')\n",
    "stations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Seleccionar algunas columnas del dataset stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = stations[['Sigla', 'Latitud', 'Longitud']] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Cambiar el nombre de una columna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = stations.rename(columns={'Sigla': 'Station'}) #Columna Sigla por Station\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Convertir las coordenadas de texto a decimales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def dms2dd(degrees, minutes, seconds, direction):\n",
    "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n",
    "    if direction == 'S' or direction == 'W':\n",
    "        dd *= -1\n",
    "    return dd;\n",
    "\n",
    "def dd2dms(deg):\n",
    "    d = int(deg)\n",
    "    md = abs(deg - d) * 60\n",
    "    m = int(md)\n",
    "    sd = (md - m) * 60\n",
    "    return [d, m, sd]\n",
    "\n",
    "def parse_dms(coor):\n",
    "    parts = re.split('[^\\d\\w]+', coor)\n",
    "    dec_coor = dms2dd(parts[0], parts[1], float(parts[2]+'.'+parts[2]), parts[4])\n",
    "    return dec_coor\n",
    "\n",
    "'''\n",
    "def dms2dd(degrees, minutes, seconds, direction):\n",
    "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n",
    "    if direction == 'E' or direction == 'S':\n",
    "        dd *= -1\n",
    "    return dd;\n",
    "\n",
    "def dd2dms(deg):\n",
    "    d = int(deg)\n",
    "    md = abs(deg - d) * 60\n",
    "    m = int(md)\n",
    "    sd = (md - m) * 60\n",
    "    return [d, m, sd]\n",
    "\n",
    "def parse_dms(dms):\n",
    "    parts = re.split('[^\\d\\w]+', dms)\n",
    "    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n",
    " \n",
    "    return (lat)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar la funcion parse_dms a la columna Latitud y Longitud\n",
    "stations['Latitud'] = stations['Latitud'].apply(parse_dms)\n",
    "stations['Longitud'] = stations['Longitud'].apply(parse_dms)\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Integrar el dataset df con el dataset stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, stations, on='Station', how='inner')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Convertir hora (DateTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Puede ver que en la columna 'DateTime', \n",
    "#la información sobre la fecha y la hora se dan juntas. \n",
    "#Por lo tanto, extraerá la información de tiempo.\n",
    "\n",
    "def replace24(datetimex):\n",
    "    return datetimex.replace('24:00', '00:00') #cambia de horarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda extraerá información de la columna 'datetime' y \n",
    "#generará columnas de meses, días o semanas y horas\n",
    "df['DateTime'] = df['DateTime'].apply(replace24)\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'], dayfirst=True)\n",
    "df['month'] = pd.DatetimeIndex(df['DateTime']).month\n",
    "df['day_week'] = pd.DatetimeIndex(df['DateTime']).weekday\n",
    "df['day_month'] = pd.DatetimeIndex(df['DateTime']).day\n",
    "df['hour'] = pd.DatetimeIndex(df['DateTime']).hour\n",
    "df.loc[df['hour']==0,'hour'] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Guardar todo el proceso en un nuevo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/dataset_with_geo_missing.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varias columnas tienen valores perdidos en el dataset\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Remplazar los valores NaN en cada columna con su media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PM10'].fillna((df['PM10'].mean()), inplace=True)\n",
    "df['PM2.5'].fillna((df['PM2.5'].mean()), inplace=True)\n",
    "df['CO'].fillna((df['CO'].mean()), inplace=True)\n",
    "df['NO'].fillna((df['NO'].mean()), inplace=True)\n",
    "df['NO2'].fillna((df['NO2'].mean()), inplace=True)\n",
    "df['NOX'].fillna((df['NOX'].mean()), inplace=True)\n",
    "df['OZONO'].fillna((df['OZONO'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora ya no hay valores perdidos en el dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Guardar todo el proceso en un nuevo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/dataset_final_clean_mean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. creando el esquema de la bodega de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset_final_clean_mean.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Se crea un nuevo DataFrame con los datos de los polutantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polutante=pd.DataFrame(df, columns=[\"PM10\", \"PM2.5\", \"NO\", \"NO2\", \"NOX\", \"CO\", \"OZONO\"]) # se crea un nuevo DataFrame unicamente con los polutantes\n",
    "df['id_polutante'] = df.index+1 # se crea una nueva columna que identificará a cada polutante\n",
    "df_polutante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Se crea un nuevo dataset con los datos de las estaciones y se eliminan las filas repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estacion=pd.DataFrame(df, columns=[\"Name\", \"Station\", \"Localidad\", \"Latitud\", \"Longitud\"]) # se crea un nuevo DataFrame unicamente con los datos de las estaciones\n",
    "df_estacion = df_estacion.drop_duplicates() # se eliminan las filas repetidas\n",
    "df_estacion = df_estacion.rename(columns={'Station': 'Sigla'}) # se cambia el nombre a la columna Station por Sigla\n",
    "df_estacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se remplaza la columna sigla, dejando unicamente los datos unicos y luego se le cambia el nombre\n",
    "# a la misma por id_estacion\n",
    "count = 1\n",
    "for index, row in df_estacion.iterrows():\n",
    "    df = df.replace({row[\"Sigla\"]: count})\n",
    "    count += 1\n",
    "df = df.rename(columns={'Station': 'id_estacion'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se crea un nuevo DataFrame con los datos de las fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea un nuevo DataFrame y se eliminan los valores repetidos\n",
    "df_fecha=pd.DataFrame(df, columns=[\"DateTime\"]) # se crea un nuevo DataFrame con las fechas y horas \n",
    "#df_fecha[\"DateTime\"].value_counts() # !TODO Borrar?\n",
    "df_fecha = df_fecha.drop_duplicates() # se eliminan las filas repetidas\n",
    "df_fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # libreria para el uso de expresiones regulares\n",
    "from pandas.tseries.holiday import Holiday, AbstractHolidayCalendar\n",
    "from datetime import datetime\n",
    "\n",
    "# definimos los días festivo para el año en que se tomaron los datos\n",
    "class EsBusinessCalendar(AbstractHolidayCalendar):\n",
    "   rules = [ # se definen los festivos, teniendo en cuenta los horarios festivos para el año de 2021 en Colombia\n",
    "     Holiday('Año Nuevo', month=1, day=1),\n",
    "     Holiday('Día de los Reyes Magos', month=1, day=11),\n",
    "     Holiday('Día de San José', month=3, day=22),\n",
    "     Holiday('Jueves Santo', month=4, day=1),\n",
    "     Holiday('Viernes Santo', month=4, day=2),\n",
    "     Holiday('Día del Trabajador', month=5, day=1),\n",
    "     Holiday('Día de la Ascensión', month=5, day=17),\n",
    "     Holiday('Corpus Christi', month=6, day=7),\n",
    "     Holiday('Sagrado Corazón', month=6, day=14),\n",
    "     Holiday('San Pedro y San Pablo ', month=7, day=5),\n",
    "     Holiday('Día de la Independencia', month=7, day=20),\n",
    "     Holiday('Batalla de Boyacá', month=8, day=7),\n",
    "     Holiday('Asunción de la Virgen', month=8, day=16),\n",
    "     Holiday('Celebración del Día de la Raza', month=10, day=18),\n",
    "     Holiday('Día de todos los Santos', month=11, day=1),\n",
    "     Holiday('Independencia de Cartagena', month=11, day=15),\n",
    "     Holiday('Inmaculada Concepción', month=12, day=8),    \n",
    "     Holiday('Navidad', month=12, day=25)\n",
    "   ]\n",
    "\n",
    "calendar_festivos = EsBusinessCalendar() # se instancia el objeto\n",
    "calendar_festivos = calendar_festivos.holidays(start='2021-01-01', end='2021-12-31') # se define el rango de tiempo en que se tendran en cuenta los festivos (anho 2021)\n",
    "\n",
    "dias = []\n",
    "meses = []\n",
    "anhos = []\n",
    "horas = []\n",
    "fin_semana = []\n",
    "festivo = []\n",
    "\n",
    "\n",
    "for index, row in df_fecha.iterrows(): # se recorre cada fila del DataFrame\n",
    "    list_fecha = re.split(\"[\\-\\s]\", row['DateTime']) # \n",
    "    dias.append(list_fecha[2])\n",
    "    meses.append(list_fecha[1])\n",
    "    anhos.append(list_fecha[0])\n",
    "    horas.append(list_fecha[3])\n",
    "\n",
    "    hora = row['DateTime'].split(\" \")\n",
    "    hora[0] = datetime.strptime(hora[0], '%Y-%m-%d')\n",
    "    if hora[0].weekday() < 5 : # condicción para determinar si el día está comprendido entre lues-viernes\n",
    "        fin_semana.append(False)\n",
    "    else:\n",
    "        fin_semana.append(True)\n",
    "    if hora[0] in calendar_festivos: # condicción para determinar si el día hace parte de los días festivos del año\n",
    "        festivo.append(True)\n",
    "    else: \n",
    "        festivo.append(False)\n",
    "\n",
    "dict_fechas = { 'dia': dias, 'mes': meses, 'anho': anhos, 'hora': horas, 'fin_semana': fin_semana, 'festivo': festivo}\n",
    "df_fechas = pd.DataFrame(data=dict_fechas)\n",
    "df_fechas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la columna id_tiempo para identificar cada una de las fechas en el DF principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "# se recorre cada fecha de df_fecha\n",
    "for index, row in df_fecha.iterrows(): \n",
    "# por cada fila de df_fecha se crea un nuevo identificador en el DataFrame principal\n",
    "    df.loc[df['DateTime'] == row[\"DateTime\"], 'DateTime'] = count     count += 1\n",
    "df = df.rename(columns={'DateTime': 'id_tiempo'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalmente se crea un DataFrame, el cual se relaciona con los DataFrame creados anteriormente por medio de sus identificadores \n",
    "df_fact_medidad=pd.DataFrame(df, columns=[\"id_estacion\", \"id_tiempo\", \"id_polutante\"])\n",
    "df_fact_medidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de las tablas en Postgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conexion import new_model\n",
    "\n",
    "# se llama a la función new_model (archivo conexion.py) con cada uno de los DataFrame para crear las tablas correspondientes\n",
    "df_fact_medidad = df_fact_medidad.astype(int)\n",
    "new_model(df_estacion, \"dim_estacion\")\n",
    "new_model(df_polutante, \"dim_polutante\")\n",
    "new_model(df_fechas, \"dim_tiempo\")\n",
    "new_model(df_fact_medidad, \"fact_medidad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
